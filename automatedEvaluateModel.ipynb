{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matt\\anaconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "nltk.download('wordnet')  # Required for METEOR score\n",
    "\n",
    "# Load JSON configurations\n",
    "CONFIG_FILE = \"evaluationConfig.json\"\n",
    "EVAL_RESULTS_FILE = \"evaluationResults.txt\"\n",
    "\n",
    "def load_configs(config_file):\n",
    "    with open(config_file, \"r\") as f:\n",
    "        configs = json.load(f)\n",
    "    return configs\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "def load_model(model_name):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "# Preprocess test data\n",
    "def preprocess_data(test_data, tokenizer):\n",
    "    sources = [\">>jpn<< \" + example['src'] for example in test_data]\n",
    "    references = [[example['trg']] for example in test_data]\n",
    "    return sources, references\n",
    "\n",
    "# Generate translations using the model with dynamic parameters\n",
    "def generate_translations(model, tokenizer, sources, config, device):\n",
    "    translations = []\n",
    "    for source in sources:\n",
    "        inputs = tokenizer(source, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Ensure only provided parameters are passed\n",
    "        generate_args = {key: value for key, value in config.items() if hasattr(model.config, key)}\n",
    "        outputs = model.generate(**inputs, **generate_args)\n",
    "        \n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, tokenizer, test_data, config, device):\n",
    "    sources, references = preprocess_data(test_data, tokenizer)\n",
    "    translations = generate_translations(model, tokenizer, sources, config, device)\n",
    "\n",
    "    bleu_score = corpus_bleu(translations, references).score\n",
    "    \n",
    "    tokenized_translations = [trans.split() for trans in translations]\n",
    "    tokenized_references = [[ref.split() for ref in ref_list] for ref_list in references]\n",
    "\n",
    "\n",
    "    meteor_scores = [\n",
    "        max(meteor_score([ref], trans) for ref in ref_list)\n",
    "        for ref_list, trans in zip(tokenized_references, tokenized_translations)\n",
    "    ]\n",
    "    avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    \n",
    "    return bleu_score, avg_meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load configurations\n",
    "configs = load_configs(CONFIG_FILE)\n",
    "\n",
    "# Load dataset\n",
    "data = load_dataset(\"NilanE/ParallelFiction-Ja_En-100k\", split=\"train\")\n",
    "dataset = data.train_test_split(test_size=0.1, seed=42)\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matt\\anaconda3\\envs\\cuda\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "original_model, tokenizer, device = load_model(\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "fine_tuned_model, _, _ = load_model(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating configurations...\n",
      "Evaluating Configuration 1...\n",
      "max_new_tokens=512, num_beams=2, do_sample=True, top_k=50, top_p=0.9, temperature=1.0\n",
      "Evaluating Configuration 1...\n",
      "max_new_tokens=512, num_beams=2, do_sample=True, top_k=50, top_p=0.9, temperature=1.0\n",
      "  - Helsinki-NLP/opus-mt-mul-en\n",
      "    - BLEU Score: 0.00000\n",
      "    - METEOR Score: 0.00816\n",
      "  - Fine-tuned Model\n",
      "    - BLEU Score: 44.98160\n",
      "    - METEOR Score: 0.11698\n",
      "==================================================\n",
      "\n",
      "Evaluating Configuration 2...\n",
      "max_new_tokens=128, num_beams=4, early_stopping=True\n",
      "Evaluating Configuration 2...\n",
      "max_new_tokens=128, num_beams=4, early_stopping=True\n",
      "  - Helsinki-NLP/opus-mt-mul-en\n",
      "    - BLEU Score: 0.00000\n",
      "    - METEOR Score: 0.00473\n",
      "  - Fine-tuned Model\n",
      "    - BLEU Score: 20.90445\n",
      "    - METEOR Score: 0.10118\n",
      "==================================================\n",
      "\n",
      "Evaluating Configuration 3...\n",
      "max_new_tokens=256, num_beams=4, no_repeat_ngram_size=3, repetition_penalty=1.2, length_penalty=1.0\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for each configuration\n",
    "print(\"Evaluating configurations...\")\n",
    "with open(EVAL_RESULTS_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "    for i, config in enumerate(configs):\n",
    "        # Prepare a string listing the parameters\n",
    "        params_str = \", \".join(f\"{k}={v}\" for k, v in config.items())\n",
    "\n",
    "        print(f\"Evaluating Configuration {i + 1}...\")\n",
    "        print(params_str)\n",
    "\n",
    "        # Evaluate original model\n",
    "        bleu_orig, meteor_orig = evaluate_model(original_model, tokenizer, test_data, config, device)\n",
    "        # Evaluate fine-tuned model\n",
    "        bleu_finetuned, meteor_finetuned = evaluate_model(fine_tuned_model, tokenizer, test_data, config, device)\n",
    "\n",
    "        # Write results in desired format\n",
    "        f.write(f\"{params_str}\\n\")\n",
    "        f.write(\"    - Helsinki-NLP/opus-mt-mul-en\\n\")\n",
    "        f.write(f\"        - BLEU Score: {bleu_orig:.5f}\\n\")\n",
    "        f.write(f\"        - METEOR Score: {meteor_orig:.5f}\\n\\n\")\n",
    "\n",
    "        f.write(\"    - Fine-tuned Model\\n\")\n",
    "        f.write(f\"        - BLEU Score: {bleu_finetuned:.5f}\\n\")\n",
    "        f.write(f\"        - METEOR Score: {meteor_finetuned:.5f}\\n\\n\")\n",
    "\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        # Optionally, also print to console\n",
    "        print(f\"Evaluating Configuration {i + 1}...\")\n",
    "        print(params_str)\n",
    "        print(\"  - Helsinki-NLP/opus-mt-mul-en\")\n",
    "        print(f\"    - BLEU Score: {bleu_orig:.5f}\")\n",
    "        print(f\"    - METEOR Score: {meteor_orig:.5f}\")\n",
    "        print(\"  - Fine-tuned Model\")\n",
    "        print(f\"    - BLEU Score: {bleu_finetuned:.5f}\")\n",
    "        print(f\"    - METEOR Score: {meteor_finetuned:.5f}\")\n",
    "        print(\"=\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
