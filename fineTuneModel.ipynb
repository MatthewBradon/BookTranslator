{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import nltk\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "nltk.download('wordnet')  # Required for METEOR score\n",
    "import random\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-ja-en\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "\n",
    "data = load_dataset(\"NilanE/ParallelFiction-Ja_En-100k\", split=\"train\")\n",
    "\n",
    "dataset = data.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Extract Japanese source text and English target text\n",
    "    inputs = examples['src']  # Japanese text\n",
    "    targets = examples['trg']  # English text\n",
    "\n",
    "    # Tokenize the source text\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Tokenize the target text as labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess test data\n",
    "def preprocess_data(test_data, tokenizer):\n",
    "    sources = []\n",
    "    references = []\n",
    "    for example in test_data:\n",
    "        sources.append(example['src'])\n",
    "        references.append([example['trg']])  # Wrap in a list for sacrebleu compatibility\n",
    "    return sources, references\n",
    "\n",
    "# Function to generate translations using the model\n",
    "def generate_translations(model, tokenizer, sources):\n",
    "    translations = []\n",
    "    for source in sources:\n",
    "        inputs = tokenizer(source, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            num_beams=4,\n",
    "            length_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,   \n",
    "            tempature = 0.6\n",
    "        )\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data):\n",
    "    # Preprocess data\n",
    "    sources, references = preprocess_data(test_data, tokenizer)\n",
    "    translations = generate_translations(model, tokenizer, sources)\n",
    "\n",
    "    # BLEU score (using raw text)\n",
    "    bleu_score = corpus_bleu(translations, references).score\n",
    "\n",
    "    # Tokenize translations and references for METEOR\n",
    "    tokenized_translations = [trans.split() for trans in translations]\n",
    "    tokenized_references = [[ref.split() for ref in ref_list] for ref_list in references]\n",
    "\n",
    "    # METEOR score\n",
    "    meteor_scores = [\n",
    "        max(meteor_score([ref], trans) for ref in ref_list)\n",
    "        for ref_list, trans in zip(tokenized_references, tokenized_translations)\n",
    "    ]\n",
    "    avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "    return bleu_score, avg_meteor_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|██████████| 95443/95443 [00:01<00:00, 48119.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': '迷子のハイエルフ8\\n転移ゲートに辿り着いてしまえばすぐにハイドラ王国である。今日はイベントのシーズンでもないのでゲートはそんなに混んでおらず、対して待ち時間も無く転移できた。\\nハイドラ王国に付くとすぐに独特な海辺の匂いと喧騒が伝わってくる。\\n「やっぱりハイドラ王国は何度きても賑やかですね」\\n「巨大な港町と表現するのが正しいかもしれないね。実際海にほど近い市場は連日かなり賑やかなものさ。ハイドラ王国はいろいろ制度が革新的で、特に税金の比率が他に比べて低く個人で店を気軽に出しやすい条件が整っており、商売を始めやすいのが大きいね。ただ、その性質上個人から中小レベルの規模の店が多くなりがちで、大商会という規模のものは少ない傾向にあるね」\\n「なるほど、確かにそんな雰囲気ですね」\\nハイドラ王国は市民の力が強く、逆に貴族はほぼ名ばかりであまり権力を持っていないという状態らしい。いずれは貴族自体が無くなるのではないかとも言われており、やはり人界の中でも一番時代の先端を行く国のようだった。\\n異世界出身の俺にとっては、比較的馴染みやすい気風かもしれない。\\nそんなことを考えながら道を歩いていると、向こうから見覚えのあるおじいさんが歩いてきた。最初に会った時の変装しているラグナさんである。\\n「フォルスさん、迎えが来たみたいですよ」\\n「ああ、そのようだね。相変わらず変身魔法は下手だね......それなりに魔法に詳しいものが見ればすぐに違和感に気付くよ。まぁ、彼女はゴリッゴリの前衛タイプだから仕方ないと言えば仕方ないが......」\\n「ノインさんも認識阻害とかは苦手って言ってましたね」\\n「まぁ、魔法というものは得手不得手があるからね。私も賢者などという呼び名で呼ばれてはいるが、あらゆる魔法が使えたりというわけでもないさ。自然魔法や大地の魔法に特化していると言っていい。逆に火などの魔法は苦手だね」\\nフォルスさんもラグナさんにはすぐ気付いたようで苦笑を浮かべていた。すると、ラグナさんの方も俺たちが気付いたことが分かったみたいで、軽く指で俺たちに合図を送ってきた。\\nたぶんあの指の先の人気のない場所に行こうという合図だと思う。ラグナさんはとんでもない支持率を誇る国王であり、ハイドラ王国内では相当人気のある存在だ。\\n実際俺と初めて会った時も変身魔法を使っていたし、素の格好で外を出歩くと大変なんだろう。\\n変装したラグナさんについて移動して、いくつかの路地を通って人気のない海辺にやってくると、ラグナさんは変装を解いた。\\n「ふぅ......カイト、すまんな。迷惑をかけた......フォルス!! 貴様、出歩くなと言うたじゃろうが!!」\\n「お、落ち着きたまえラグナ......と、とりあえず、振り上げた拳を降ろすんだ。いいか、すぐに暴力に訴えるのは野蛮な獣の行いだ。我々は理性ある生物だ......怒りに流されてはいけない」\\n「アレだけ前もって釘を刺したのに迷子になっておっては、怒りもしよう。そもそも......うん? ほぅ......ふむ......」\\n「なんだい? ラグナ、嫌な目をしているね......その目は主に、揶揄ったりする際の目だろう? いまなぜそのような目を私に対して向けているのか、説明を求めたいところではあるが......悲しいかな若干の心当たりがあるので、あまり触れてほしくないという気持ちもある。触れるのであれば、ある程度の覚悟はしていただきたいね」\\nなにやらよく分からないやり取りがあって、怒っていたラグナさんはどこかニヤニヤと意地の悪そうな笑みを浮かべる。\\nそして、なんとか話題を戻そうとするフォルスさんの抵抗をあざ笑うかのように口を開いた。\\n「......フォルス、お主珍しく照れておるようじゃの? 微妙に頬に赤みが見える。ほほぅ、まさかお主のそんな顔が見られるとは......原因は、カイトというわけか。ははは、流石と褒めるべきじゃな! あのフォルスが、そんな顔――うぉっ!? ちょっ、待てっ!?」\\n「警告はしたよ......」\\n楽し気にフォルスさんを揶揄っていたラグナさんだったが、突如地面がまるで沼のように変化して体が沈み、抵抗しようとするも地面に引きずり込まれていった。\\nそして、いつの間にか木でできた杖を持っていたフォルスさんが、魔力を込めてトンと地面を叩く。\\n「......アースシェイカー」\\nすると、大きな揺れ......地震が発生して、慌てて周囲を見るが、海は波立ったりしておらず......俺たちがいる場所だけ揺れているような感じだった。\\n不思議な状態だが、たぶん周辺に被害が出ないように対抗術式で調整して超局地的な地震を起こしているのだろう。\\nそして少しして、揺れが収まると......ラグナさんが沈んだ地面が鋼鉄のようにガッチガチに固まっていた。\\n「......さて、せっかくハイドラ王国に来たのだし、王城に向かう前に軽く昼食でも食べようか。今回の礼ということで、是非奢らせてくれ」\\n「......あ、いや、ラグナさんは?」\\n「......この程度で死ぬようなら、千年前の旅で死んでいるよ。本気で固めたが、数分で出てくるだろうね。まぁ、放っておいていいさ、行こう」\\n「あ、はい」', 'trg': \"Chapter 1627: Lost High Elf 8\\nOnce we reached the Teleportation Gate, we will soon be in Hydra Kingdom. Since today wasn't a season brimming with events, the gate wasn't so crowded, and we were able to teleport without much waiting time.\\nAs soon as we arrived at Hydra Kingdom, I could smell the unique seaside scent and feel the hustle and bustle of the place.\\n[No matter how many times I visit this place, Hydra Kingdom really is always bustling.]\\n[It's probably more accurate to call this place a huge port city. In fact, the markets near the sea are always quite lively. Hydra Kingdom has various innovative systems in place, particularly with their lower tax rate compared to others. The conditions are conducive for individuals to easily open shops, making it much easier for one to start a business. However, due to this nature of the Kingdom, there tends to be a large number of small to medium-sized shops and not many large commercial establishments.]\\n[I see, it certainly has that kind of atmosphere.]\\nHydra Kingdom is a place where the authority held by the citizens is strong, while the nobles are almost all in name only and their authority isn't much greater than the others. I even heard some saying that nobility in itself may eventually disappear in the country. I guess that country really seemed to be the most ahead of its time in the Human Realm.\\nAs someone from a different world, I find this characteristic of Hydra Kingdom's culture relatively easier to get used to compared to the other countries.\\nAs we were walking down the street while I had such a thought in mind, a familiar old man walked up to me from the other side. It was Laguna-san, wearing the disguise she had when we first met.\\n[Fors-san, it looks like she's come to pick you up.]\\n[Yeah, that indeed seems to be the case. She sure is as unskillful at Transformation Magic as usual...... Those well-versed in magic would notice the discrepancies fairly quickly. Well, she's a staunch front-line type, so I suppose it's inevitable......]\\n[Neun-san also mentioned how she's not very good at Recognition Inhibition Magic either.]\\n[Well, magic is something that some are good while others aren't. I might be referred to as a Sage, but it doesn't mean I can use all sorts of magic. One could say I'm more specialized in Nature Magic and Earth Magic. On the other hand, I'm not very skilled in Fire and other such magic.]\\nFors-san also seemed to notice Laguna-san immediately as she wryly smiled. Thereupon, Laguna-san seemed to know that we had noticed her, as she lightly signaled to us with a finger.\\nShe was probably signaling us to go to an unpopulated area in the direction her finger was pointing to. Laguna-san is a King with a tremendous approval rating and is quite popular within Hydra Kingdom.\\nIn fact, when I first met her, she was also using Transformation Magic, so I imagine it would be difficult for her to walk around in the street without a disguise.\\nWe followed the disguised Laguna-san through several alleys until we came to an unpopulated seaside area, where Laguna-san took off her disguise.\\n[Fuuu...... Kaito, sorry for troubling you...... Fors!!! I told you not to go out, didn't I!!!?]\\n[C- Calm down, Laguna...... F- For the time being, put down that fist you're raising. Listen here, it's the behavior of savage beasts to immediately resort to violence. We are rational creatures...... We must not let our anger get the better of us.]\\n[Even though I've kept on warning you about this repeatedly beforehand, for you to still get lost like that, I'd obviously get mad. In the first place...... Unnn? Hou...... fumu......]\\n[What is it? Laguna, that's an unpleasant look you have there...... Those eyes, don't you have those when you're teasing me? I'm now inclined to ask for an explanation for why those eyes are directed towards me at a time like this...... but unfortunately, I do have a slight inkling as to why, so I also feel like not wanting to delve into it too much. If you're gonna touch upon this matter, I'd appreciate holding onto some resolve to a certain extent.]\\nThey had some sort of exchange that I didn't quite understand, and Laguna-san, who should have been angry just now, is now grinning like a cheshire cat.\\nThen, as if to mock Fors-san's attempt to somehow get the topic back to before, Laguna-san spoke.\\n[......Fors, aren't you being unusually shy over there? Oh my, is that a faint blush on your cheeks I see? Hohou, who would have thought I'd get to see you looking like that...... The cause must be Kaito huh. Hahaha, I must say, as to be expected of him! For that Fors to be making such a face------- Whoa!? Hey, wait!?]\\n[I did warn you......]\\nLaguna-san was happily teasing Fors-san, but the ground beneath her feet suddenly turned into something like a swamp and her body sank, and even though she tried to resist, she was dragged into the ground.\\nThen, Fors-san, who was holding a wooden staff before I knew it, imbued it with magic power and tapped its base on the ground.\\n[......Earth Shaker.]\\nThereupon, a big tremor...... an earthquake occurred, and considering we were on the seaside, I looked around in panic, but then I noticed that the sea wasn't rippling at all...... It seemed that it was only the location we were in that were shaking.\\nIt's quite a mysterious situation, but most likely, to prevent any damage to the vicinity, a counteracting magic is being employed to induce an ultra-localized earthquake.\\nAnd then, a little later, when the shaking stopped...... the ground on which Laguna-san had sunk became as hard as steel.\\n[......Well then, since we've come all the way to Hydra Kingdom, let's have a light lunch before we head to the royal castle. As thanks for this matter, allow me to treat you.]\\n[......Ah, no, what about Laguna-san?]\\n[......If she were to die from something like this, she would have died on that journey a thousand years ago. I've seriously solidified and reinforced that ground, but I'm sure she'll be out in a few minutes. Well, just leave her be. Let's go.]\\n[Ah, yes.]\", 'meta': {'general': {'series_title_eng': 'I Was Caught up in a Hero Summoning, but That World Is at Peace (WN)', 'series_title_jap': '勇者召喚に巻き込まれたけど、異世界は平和でした', 'sentence_alignment_score': 1.53}, 'novelupdates': {'link': 'https://www.novelupdates.com/series/i-was-caught-up-in-a-hero-summoning-but-that-world-is-at-peace/', 'genres': ['Comedy', 'Fantasy', 'Harem', 'Romance', 'Shounen', 'Slice of Life'], 'tags': ['Adapted to Manga', 'Beast Companions', 'Beastkin', 'Beautiful Female Lead', 'Carefree Protagonist', 'Charismatic Protagonist', 'Charming Protagonist', 'Clingy Lover', 'Clumsy Love Interests', 'Demi-Humans', 'Demon Lord', 'Demons', 'Dense Protagonist', 'Devoted Love Interests', 'Divine Protection', 'Dragons', 'Easy Going Life', 'Elves', 'Fairies', 'First-time Interc**rse', 'Gambling', 'God-human Relationship', 'Goddesses', 'Heartwarming', 'Heroes', 'Human-Nonhuman Relationship', 'Immortals', 'Kind Love Interests', 'Lack of Common Sense', 'Loli', 'Love Interest Falls in Love First', 'Loyal Subordinates', 'Magic', 'Maids', 'Male Protagonist', 'Monsters', 'Multiple POV', 'Multiple Transported Individuals', 'Naive Protagonist', 'Nobles', 'Older Love Interests', 'Past Plays a Big Role', 'Polygamy', 'R-15', 'Royalty', 'Shy Characters', 'Store Owner', 'Strong Love Interests', 'Summoned Hero', 'Sword And Magic', 'Tomboyish Female Lead', 'Transported to Another World', 'Tsundere', 'Unconditional Love', 'Vampires', 'Wealthy Characters', 'World Tree', 'Yandere', 'Younger Love Interests'], 'rating': 4.1, 'rating_votes': 636}, 'syosetu': {'link': 'https://ncode.syosetu.com/n2273dh/', 'series_active': True, 'writer': '灯台', 'fav_novel_cnt': 75937, 'global_points': 246366}}, 'input_ids': [19191, 576, 13, 8755, 46831, 1773, 24, 45604, 19812, 29, 19224, 47010, 381, 443, 4258, 1713, 29, 43834, 1544, 7532, 459, 5832, 3754, 18, 13442, 13, 31691, 5654, 13, 53, 19812, 18, 2534, 29, 17200, 401, 816, 5618, 15168, 4286, 81, 9073, 794, 109, 11001, 45604, 375, 88, 5832, 5066, 15035, 7532, 29, 22493, 42, 1713, 29, 10670, 93, 3568, 3914, 13, 12942, 42, 58095, 39497, 36707, 83, 1708, 5832, 24, 18155, 261, 9419, 43834, 1544, 7532, 18, 267, 951, 5044, 109, 58132, 5938, 74, 207, 20056, 24, 18155, 11172, 9320, 18505, 3897, 42, 5751, 154, 13, 34, 2769, 1368, 1389, 207, 5832, 3454, 3568, 29, 1469, 6939, 8810, 18, 5743, 583, 7130, 58132, 5938, 3755, 194, 5832, 43834, 1544, 7532, 18, 13383, 15467, 34, 25696, 1046, 53, 15168, 11623, 29612, 13, 32512, 34, 4288, 29, 10891, 81, 19253, 9446, 53, 3840, 22, 1287, 11886, 29, 3484, 8636, 7062, 34, 11850, 83, 4734, 15168, 20243, 22, 1823, 8636, 13, 34, 5990, 207, 5832, 2855, 15168, 425, 19502, 598, 9446, 135, 311, 3420, 6787, 13, 29974, 3840, 34, 1446, 2243, 20712, 53, 15168, 797, 14870, 1486, 504, 29974, 398, 18, 10789, 16067, 29, 392, 207, 20056, 24, 18155, 42072, 15168, 4785, 29, 2534, 33936, 74, 207, 20056, 5066, 15035, 7532, 18, 11153, 13, 835, 34, 6976, 15168, 8050, 29, 44733, 18, 16325, 1450, 3565, 53, 7125, 16996, 22, 762, 882, 72, 504, 2525, 4661, 5832, 15091, 18, 44733, 11189, 34, 37438, 13, 295, 1295, 1618, 2970, 631, 81, 4734, 15168, 32400, 220, 6711, 13, 311, 427, 3535, 4385, 13, 26995, 22, 857, 8010, 326, 61, 104, 5832, 4126, 1270, 12554, 13, 1245, 2261, 18, 15168, 12081, 1046, 34332, 8636, 1287, 2685, 1368, 1389, 5832, 1021, 153, 22, 886, 3037, 1223, 22, 4720, 125, 512, 42, 15168, 11447, 135, 1011, 3162, 13, 392, 27942, 125, 647, 34, 4720, 381, 1086, 5832, 2689, 29, 1486, 104, 371, 13, 2423, 4068, 541, 36226, 1428, 647, 459, 5832, 24, 18155, 8245, 563, 647, 15168, 9645, 34, 1101, 1254, 74, 116, 20056, 24, 18155, 8007, 15168, 425, 326, 2463, 5832, 47794, 2423, 1874, 7063, 18, 1210, 594, 2463, 70, 70, 441, 2243, 29, 7063, 29, 23097, 398, 34, 1011, 714, 1713, 29, 6347, 4104, 3863, 29, 32852, 116, 5832, 193, 1385, 15168, 894, 18, 16789, 2543, 16789, 13, 910, 13580, 6038, 1304, 36705, 1386, 4258, 36705, 34, 70, 70, 20056, 24, 18155, 3117, 5316, 647, 109, 11514, 37073, 1550, 18, 24456, 83, 912, 193, 82, 207, 20056, 24, 18155, 193, 1385, 15168, 7063, 504, 398, 18, 2250, 594, 2363, 2250, 594, 420, 135, 207, 5832, 284, 109, 33128, 208, 1699, 504, 7456, 1450, 53, 4214, 618, 81, 18, 442, 34, 15168, 7186, 7063, 34, 9147, 4260, 504, 1868, 5654, 194, 5832, 6436, 7063, 261, 8563, 13, 7063, 29, 11294, 1709, 541, 42, 912, 370, 5832, 8050, 29, 2148, 1699, 13, 7063, 18, 24456, 2463, 20056, 14345, 563, 647, 109, 36226, 1428, 647, 229, 1713, 13825, 277, 326, 53, 14004, 1928, 22, 18251, 6298, 81, 277, 5832, 1434, 15168, 36226, 1428, 647, 13, 462, 109, 1245, 389, 34, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [29002, 652, 11366, 44, 20497, 3992, 5722, 1851, 360, 3401, 59, 3444, 5, 40611, 7900, 1600, 24716, 3, 59, 78, 1092, 48, 25, 53150, 660, 2, 1467, 508, 50, 145, 12, 52, 20, 6423, 18483, 31281, 55, 2676, 3, 5, 8188, 50, 145, 12, 52, 139, 16376, 3, 19, 59, 129, 734, 8, 22816, 7900, 635, 324, 2492, 164, 2, 407, 1092, 68, 59, 3193, 102, 53150, 660, 3, 23, 212, 6636, 5, 4736, 1927, 8660, 22322, 19, 567, 5, 43144, 19, 55167, 11, 5, 503, 2, 151, 3985, 839, 228, 264, 473, 23, 2325, 60, 503, 3, 53150, 660, 331, 32, 613, 49016, 2, 171, 151, 2244, 12, 17, 1601, 185, 4233, 8, 645, 60, 503, 20, 3300, 8847, 907, 2, 189, 727, 3, 5, 16210, 1522, 5, 1927, 58, 613, 1495, 28756, 2, 53150, 660, 143, 2450, 35490, 4422, 25, 503, 3, 3594, 55, 114, 4554, 6231, 3296, 5835, 8, 489, 2, 66, 2889, 58, 40753, 2992, 39, 2896, 8, 3485, 1174, 20506, 3, 867, 36, 324, 4606, 39, 105, 8, 984, 20, 1454, 2, 841, 3, 2245, 8, 60, 2779, 11, 5, 660, 3, 142, 17533, 8, 48, 20, 1288, 612, 11, 1028, 8, 12065, 146, 17, 3459, 20506, 19, 57, 264, 1288, 7127, 15375, 17, 2, 171, 151, 411, 215, 3, 36, 1534, 143, 30, 603, 11, 5867, 2, 171, 53150, 660, 32, 20, 503, 325, 5, 2354, 2037, 96, 5, 7101, 32, 1647, 3, 653, 5, 31897, 58, 1384, 97, 25, 426, 255, 19, 114, 2354, 1397, 12, 52, 324, 1866, 242, 5, 489, 2, 23, 276, 1016, 199, 897, 30, 47134, 25, 1514, 231, 3523, 10483, 25, 5, 1173, 2, 23, 2085, 30, 1173, 331, 3148, 8, 48, 5, 412, 2238, 11, 328, 164, 25, 5, 7684, 9479, 111, 2, 407, 805, 85, 20, 710, 268, 3, 23, 385, 60, 17050, 11, 53150, 660, 12, 17, 5651, 4075, 314, 4606, 8, 196, 505, 8, 5835, 8, 5, 243, 1670, 2, 407, 59, 129, 3034, 309, 5, 3358, 653, 23, 141, 288, 20, 591, 25, 709, 3, 20, 5233, 555, 248, 5019, 150, 8, 79, 85, 5, 243, 1176, 2, 106, 50, 56946, 3641, 146, 10026, 3, 5747, 5, 32881, 239, 141, 161, 59, 301, 1809, 2, 151, 4067, 17, 146, 10026, 3, 36, 1867, 160, 239, 12, 17, 291, 8, 3144, 27, 150, 2, 171, 151, 7232, 3, 30, 1426, 1733, 8, 48, 5, 855, 2, 320, 673, 32, 68, 1225, 17, 19475, 4007, 102, 35184, 1600, 33458, 68, 8259, 70, 70, 1364, 350, 146, 21742, 195, 25, 5939, 147, 3829, 5, 23713, 121, 6465, 39060, 11032, 2571, 2, 362, 3, 239, 12, 17, 20, 46401, 1899, 146, 5127, 2139, 3, 139, 23, 5974, 36, 12, 17, 17398, 70, 70, 171, 151, 1129, 463, 3675, 146, 10026, 270, 2620, 228, 239, 12, 17, 57, 298, 224, 102, 56523, 189, 7711, 475, 11839, 33458, 1627, 2, 171, 151, 9158, 3, 5939, 32, 351, 30, 199, 58, 224, 653, 489, 3544, 12, 52, 2, 23, 484, 48, 6339, 8, 68, 20, 48044, 3, 138, 36, 257, 145, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_data = train_data.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_train_data.save_to_disk(\"./tokenized_data\")\n",
    "\n",
    "print(tokenized_train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_22360\\3868523678.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments with evaluation disabled\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",  # Disable evaluation\n",
    "    save_strategy=\"steps\",  # Save checkpoints periodically\n",
    "    save_steps=500,  # Save a checkpoint every 500 steps\n",
    "    save_total_limit=3,  # Keep only the last 3 checkpoints\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use FP16 if a GPU is available\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,  # Training data\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 14896/35793 [3:34:55<211:27:27, 36.43s/it]"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "original_model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/35793 [00:00<?, ?it/s]c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      " 42%|████▏     | 15000/35793 [08:56<4:36:31,  1.25it/s]c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[60715]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1695, 'grad_norm': 2.804398775100708, 'learning_rate': 1.1622384265079764e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 15500/35793 [16:15<4:43:53,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1699, 'grad_norm': 2.7245330810546875, 'learning_rate': 1.1343000027938424e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 16000/35793 [23:22<4:25:46,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1818, 'grad_norm': 3.016148090362549, 'learning_rate': 1.1063615790797084e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 16500/35793 [30:29<4:29:14,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1462, 'grad_norm': 2.8308584690093994, 'learning_rate': 1.0784231553655745e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17000/35793 [37:44<4:43:37,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1336, 'grad_norm': 2.7448885440826416, 'learning_rate': 1.0505406084988685e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 17500/35793 [44:52<4:14:43,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1376, 'grad_norm': 2.608849048614502, 'learning_rate': 1.0226580616321628e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18000/35793 [51:50<4:07:32,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1248, 'grad_norm': 3.023585081100464, 'learning_rate': 9.947196379180287e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 18500/35793 [58:56<3:58:44,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1282, 'grad_norm': 2.806114912033081, 'learning_rate': 9.667812142038947e-06, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19000/35793 [1:06:03<3:43:46,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1038, 'grad_norm': 2.6417856216430664, 'learning_rate': 9.388427904897607e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 19500/35793 [1:13:06<3:54:52,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.103, 'grad_norm': 2.620089054107666, 'learning_rate': 9.109043667756265e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20000/35793 [1:20:11<3:31:32,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0912, 'grad_norm': 2.9677674770355225, 'learning_rate': 8.829659430614926e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 20500/35793 [1:27:15<3:31:13,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0821, 'grad_norm': 2.5534780025482178, 'learning_rate': 8.550275193473586e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 21000/35793 [1:34:19<3:26:34,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0814, 'grad_norm': 2.926218032836914, 'learning_rate': 8.270890956332244e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 21500/35793 [1:41:27<3:20:08,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0825, 'grad_norm': 2.630681037902832, 'learning_rate': 7.991506719190904e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 22000/35793 [1:48:27<3:06:31,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0786, 'grad_norm': 2.7724485397338867, 'learning_rate': 7.712681250523846e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 22500/35793 [1:55:46<2:06:30,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0631, 'grad_norm': 2.579216480255127, 'learning_rate': 7.433855781856788e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 23000/35793 [2:00:30<1:57:29,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0617, 'grad_norm': 2.6959280967712402, 'learning_rate': 7.154471544715448e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 23500/35793 [2:05:13<1:53:22,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.059, 'grad_norm': 2.834404230117798, 'learning_rate': 6.875087307574107e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24000/35793 [2:09:55<1:50:44,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0463, 'grad_norm': 2.6310675144195557, 'learning_rate': 6.595703070432767e-06, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 24500/35793 [2:14:38<1:39:37,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0497, 'grad_norm': 2.972698450088501, 'learning_rate': 6.316318833291426e-06, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 25000/35793 [2:19:20<1:43:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0155, 'grad_norm': 2.623328924179077, 'learning_rate': 6.036934596150086e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 25500/35793 [2:24:06<1:35:22,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0197, 'grad_norm': 2.651944160461426, 'learning_rate': 5.757550359008745e-06, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 26000/35793 [2:28:46<1:33:18,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.013, 'grad_norm': 2.514033317565918, 'learning_rate': 5.478724890341688e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 26500/35793 [2:33:27<1:26:46,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0212, 'grad_norm': 2.7133944034576416, 'learning_rate': 5.1993406532003465e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 27000/35793 [2:38:10<1:20:44,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.02, 'grad_norm': 2.6151833534240723, 'learning_rate': 4.919956416059007e-06, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 27500/35793 [2:42:51<56:15,  2.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0149, 'grad_norm': 2.805305242538452, 'learning_rate': 4.640572178917666e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28000/35793 [2:47:27<1:10:19,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0082, 'grad_norm': 2.6519854068756104, 'learning_rate': 4.361187941776325e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 28500/35793 [2:52:05<1:07:19,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0096, 'grad_norm': 2.5853476524353027, 'learning_rate': 4.082362473109268e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 29000/35793 [2:56:43<1:00:55,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0116, 'grad_norm': 2.951279878616333, 'learning_rate': 3.8029782359679268e-06, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 29500/35793 [3:01:21<58:55,  1.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.006, 'grad_norm': 2.7072432041168213, 'learning_rate': 3.5235939988265865e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 30000/35793 [3:05:57<51:43,  1.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0026, 'grad_norm': 2.556507110595703, 'learning_rate': 3.2442097616852458e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 30500/35793 [3:10:34<48:50,  1.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9802, 'grad_norm': 2.8558201789855957, 'learning_rate': 2.9648255245439055e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 31000/35793 [3:15:11<46:04,  1.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9977, 'grad_norm': 2.7457668781280518, 'learning_rate': 2.685441287402565e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 31500/35793 [3:19:47<38:52,  1.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0016, 'grad_norm': 2.637615442276001, 'learning_rate': 2.4060570502612245e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 32000/35793 [3:24:24<35:46,  1.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9958, 'grad_norm': 2.698451519012451, 'learning_rate': 2.126672813119884e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 32500/35793 [3:29:01<29:11,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.007, 'grad_norm': 2.9999020099639893, 'learning_rate': 1.8472885759785433e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33000/35793 [3:33:38<25:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0048, 'grad_norm': 2.849747657775879, 'learning_rate': 1.5679043388372028e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 33500/35793 [3:38:18<21:43,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9991, 'grad_norm': 2.555652618408203, 'learning_rate': 1.289078870170145e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 34000/35793 [3:42:56<17:05,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9842, 'grad_norm': 2.5811784267425537, 'learning_rate': 1.0096946330288046e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 34500/35793 [3:47:32<11:38,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9881, 'grad_norm': 2.5673716068267822, 'learning_rate': 7.303103958874641e-07, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 35000/35793 [3:52:04<07:16,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9847, 'grad_norm': 2.497488498687744, 'learning_rate': 4.509261587461236e-07, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 35500/35793 [3:56:40<02:39,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9825, 'grad_norm': 2.5490450859069824, 'learning_rate': 1.7154192160478308e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35793/35793 [3:59:23<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14363.9178, 'train_samples_per_second': 19.934, 'train_steps_per_second': 2.492, 'train_loss': 1.2199590257369632, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model\\\\tokenizer_config.json',\n",
       " './fine_tuned_model\\\\special_tokens_map.json',\n",
       " './fine_tuned_model\\\\vocab.json',\n",
       " './fine_tuned_model\\\\source.spm',\n",
       " './fine_tuned_model\\\\target.spm',\n",
       " './fine_tuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "original_model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 199.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m original_model \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m bleu, meteor \u001b[38;5;241m=\u001b[39m evaluate_model(original_model, tokenizer, test_data)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3163\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 199.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "test_data = test_data.shuffle(seed=42).select(range(100))\n",
    "\n",
    "original_model = original_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleu, meteor = evaluate_model(original_model, tokenizer, test_data)\n",
    "\n",
    "print(f\"BLEU Score: {bleu:.2f}\")\n",
    "print(f\"METEOR Score: {meteor:.2f}\")\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_model\")\n",
    "finetuned_model = finetuned_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleu, meteor = evaluate_model(finetuned_model, tokenizer, test_data)\n",
    "\n",
    "print(f\"BLEU Score: {bleu:.2f}\")\n",
    "print(f\"METEOR Score: {meteor:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EpubTranslatorEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
