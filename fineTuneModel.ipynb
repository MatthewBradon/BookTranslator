{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import nltk\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "nltk.download('wordnet')  # Required for METEOR score\n",
    "import random\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-ja-en\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "\n",
    "data = load_dataset(\"NilanE/ParallelFiction-Ja_En-100k\", split=\"train\")\n",
    "\n",
    "dataset = data.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Extract Japanese source text and English target text\n",
    "    inputs = examples['src']  # Japanese text\n",
    "    targets = examples['trg']  # English text\n",
    "\n",
    "    # Tokenize the source text\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Tokenize the target text as labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess test data\n",
    "def preprocess_data(test_data, tokenizer):\n",
    "    sources = []\n",
    "    references = []\n",
    "    for example in test_data:\n",
    "        sources.append(example['src'])\n",
    "        references.append([example['trg']])  # Wrap in a list for sacrebleu compatibility\n",
    "    return sources, references\n",
    "\n",
    "# Function to generate translations using the model\n",
    "def generate_translations(model, tokenizer, sources):\n",
    "    translations = []\n",
    "    for source in sources:\n",
    "        inputs = tokenizer(source, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            num_beams=4,\n",
    "            length_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,   \n",
    "            tempature = 0.6\n",
    "        )\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data):\n",
    "    # Preprocess data\n",
    "    sources, references = preprocess_data(test_data, tokenizer)\n",
    "    translations = generate_translations(model, tokenizer, sources)\n",
    "\n",
    "    # BLEU score (using raw text)\n",
    "    bleu_score = corpus_bleu(translations, references).score\n",
    "\n",
    "    # Tokenize translations and references for METEOR\n",
    "    tokenized_translations = [trans.split() for trans in translations]\n",
    "    tokenized_references = [[ref.split() for ref in ref_list] for ref_list in references]\n",
    "\n",
    "    # METEOR score\n",
    "    meteor_scores = [\n",
    "        max(meteor_score([ref], trans) for ref in ref_list)\n",
    "        for ref_list, trans in zip(tokenized_references, tokenized_translations)\n",
    "    ]\n",
    "    avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "    return bleu_score, avg_meteor_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95443/95443 [00:01<00:00, 48119.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': 'è¿·å­ã®ãƒã‚¤ã‚¨ãƒ«ãƒ•8\\nè»¢ç§»ã‚²ãƒ¼ãƒˆã«è¾¿ã‚Šç€ã„ã¦ã—ã¾ãˆã°ã™ãã«ãƒã‚¤ãƒ‰ãƒ©çŽ‹å›½ã§ã‚ã‚‹ã€‚ä»Šæ—¥ã¯ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚·ãƒ¼ã‚ºãƒ³ã§ã‚‚ãªã„ã®ã§ã‚²ãƒ¼ãƒˆã¯ãã‚“ãªã«æ··ã‚“ã§ãŠã‚‰ãšã€å¯¾ã—ã¦å¾…ã¡æ™‚é–“ã‚‚ç„¡ãè»¢ç§»ã§ããŸã€‚\\nãƒã‚¤ãƒ‰ãƒ©çŽ‹å›½ã«ä»˜ãã¨ã™ãã«ç‹¬ç‰¹ãªæµ·è¾ºã®åŒ‚ã„ã¨å–§é¨’ãŒä¼ã‚ã£ã¦ãã‚‹ã€‚\\nã€Œã‚„ã£ã±ã‚Šãƒã‚¤ãƒ‰ãƒ©çŽ‹å›½ã¯ä½•åº¦ãã¦ã‚‚è³‘ã‚„ã‹ã§ã™ã­ã€\\nã€Œå·¨å¤§ãªæ¸¯ç”ºã¨è¡¨ç¾ã™ã‚‹ã®ãŒæ­£ã—ã„ã‹ã‚‚ã—ã‚Œãªã„ã­ã€‚å®Ÿéš›æµ·ã«ã»ã©è¿‘ã„å¸‚å ´ã¯é€£æ—¥ã‹ãªã‚Šè³‘ã‚„ã‹ãªã‚‚ã®ã•ã€‚ãƒã‚¤ãƒ‰ãƒ©çŽ‹å›½ã¯ã„ã‚ã„ã‚åˆ¶åº¦ãŒé©æ–°çš„ã§ã€ç‰¹ã«ç¨Žé‡‘ã®æ¯”çŽ‡ãŒä»–ã«æ¯”ã¹ã¦ä½Žãå€‹äººã§åº—ã‚’æ°—è»½ã«å‡ºã—ã‚„ã™ã„æ¡ä»¶ãŒæ•´ã£ã¦ãŠã‚Šã€å•†å£²ã‚’å§‹ã‚ã‚„ã™ã„ã®ãŒå¤§ãã„ã­ã€‚ãŸã ã€ãã®æ€§è³ªä¸Šå€‹äººã‹ã‚‰ä¸­å°ãƒ¬ãƒ™ãƒ«ã®è¦æ¨¡ã®åº—ãŒå¤šããªã‚ŠãŒã¡ã§ã€å¤§å•†ä¼šã¨ã„ã†è¦æ¨¡ã®ã‚‚ã®ã¯å°‘ãªã„å‚¾å‘ã«ã‚ã‚‹ã­ã€\\nã€Œãªã‚‹ã»ã©ã€ç¢ºã‹ã«ãã‚“ãªé›°å›²æ°—ã§ã™ã­ã€\\nãƒã‚¤ãƒ‰ãƒ©çŽ‹å›½ã¯å¸‚æ°‘ã®åŠ›ãŒå¼·ãã€é€†ã«è²´æ—ã¯ã»ã¼åã°ã‹ã‚Šã§ã‚ã¾ã‚Šæ¨©åŠ›ã‚’æŒã£ã¦ã„ãªã„ã¨ã„ã†çŠ¶æ…‹ã‚‰ã—ã„ã€‚ã„ãšã‚Œã¯è²´æ—è‡ªä½“ãŒç„¡ããªã‚‹ã®ã§ã¯ãªã„ã‹ã¨ã‚‚è¨€ã‚ã‚Œã¦ãŠã‚Šã€ã‚„ã¯ã‚Šäººç•Œã®ä¸­ã§ã‚‚ä¸€ç•ªæ™‚ä»£ã®å…ˆç«¯ã‚’è¡Œãå›½ã®ã‚ˆã†ã ã£ãŸã€‚\\nç•°ä¸–ç•Œå‡ºèº«ã®ä¿ºã«ã¨ã£ã¦ã¯ã€æ¯”è¼ƒçš„é¦´æŸ“ã¿ã‚„ã™ã„æ°—é¢¨ã‹ã‚‚ã—ã‚Œãªã„ã€‚\\nãã‚“ãªã“ã¨ã‚’è€ƒãˆãªãŒã‚‰é“ã‚’æ­©ã„ã¦ã„ã‚‹ã¨ã€å‘ã“ã†ã‹ã‚‰è¦‹è¦šãˆã®ã‚ã‚‹ãŠã˜ã„ã•ã‚“ãŒæ­©ã„ã¦ããŸã€‚æœ€åˆã«ä¼šã£ãŸæ™‚ã®å¤‰è£…ã—ã¦ã„ã‚‹ãƒ©ã‚°ãƒŠã•ã‚“ã§ã‚ã‚‹ã€‚\\nã€Œãƒ•ã‚©ãƒ«ã‚¹ã•ã‚“ã€è¿ŽãˆãŒæ¥ãŸã¿ãŸã„ã§ã™ã‚ˆã€\\nã€Œã‚ã‚ã€ãã®ã‚ˆã†ã ã­ã€‚ç›¸å¤‰ã‚ã‚‰ãšå¤‰èº«é­”æ³•ã¯ä¸‹æ‰‹ã ã­......ãã‚Œãªã‚Šã«é­”æ³•ã«è©³ã—ã„ã‚‚ã®ãŒè¦‹ã‚Œã°ã™ãã«é•å’Œæ„Ÿã«æ°—ä»˜ãã‚ˆã€‚ã¾ãã€å½¼å¥³ã¯ã‚´ãƒªãƒƒã‚´ãƒªã®å‰è¡›ã‚¿ã‚¤ãƒ—ã ã‹ã‚‰ä»•æ–¹ãªã„ã¨è¨€ãˆã°ä»•æ–¹ãªã„ãŒ......ã€\\nã€ŒãƒŽã‚¤ãƒ³ã•ã‚“ã‚‚èªè­˜é˜»å®³ã¨ã‹ã¯è‹¦æ‰‹ã£ã¦è¨€ã£ã¦ã¾ã—ãŸã­ã€\\nã€Œã¾ãã€é­”æ³•ã¨ã„ã†ã‚‚ã®ã¯å¾—æ‰‹ä¸å¾—æ‰‹ãŒã‚ã‚‹ã‹ã‚‰ã­ã€‚ç§ã‚‚è³¢è€…ãªã©ã¨ã„ã†å‘¼ã³åã§å‘¼ã°ã‚Œã¦ã¯ã„ã‚‹ãŒã€ã‚ã‚‰ã‚†ã‚‹é­”æ³•ãŒä½¿ãˆãŸã‚Šã¨ã„ã†ã‚ã‘ã§ã‚‚ãªã„ã•ã€‚è‡ªç„¶é­”æ³•ã‚„å¤§åœ°ã®é­”æ³•ã«ç‰¹åŒ–ã—ã¦ã„ã‚‹ã¨è¨€ã£ã¦ã„ã„ã€‚é€†ã«ç«ãªã©ã®é­”æ³•ã¯è‹¦æ‰‹ã ã­ã€\\nãƒ•ã‚©ãƒ«ã‚¹ã•ã‚“ã‚‚ãƒ©ã‚°ãƒŠã•ã‚“ã«ã¯ã™ãæ°—ä»˜ã„ãŸã‚ˆã†ã§è‹¦ç¬‘ã‚’æµ®ã‹ã¹ã¦ã„ãŸã€‚ã™ã‚‹ã¨ã€ãƒ©ã‚°ãƒŠã•ã‚“ã®æ–¹ã‚‚ä¿ºãŸã¡ãŒæ°—ä»˜ã„ãŸã“ã¨ãŒåˆ†ã‹ã£ãŸã¿ãŸã„ã§ã€è»½ãæŒ‡ã§ä¿ºãŸã¡ã«åˆå›³ã‚’é€ã£ã¦ããŸã€‚\\nãŸã¶ã‚“ã‚ã®æŒ‡ã®å…ˆã®äººæ°—ã®ãªã„å ´æ‰€ã«è¡Œã“ã†ã¨ã„ã†åˆå›³ã ã¨æ€ã†ã€‚ãƒ©ã‚°ãƒŠã•ã‚“ã¯ã¨ã‚“ã§ã‚‚ãªã„æ”¯æŒçŽ‡ã‚’èª‡ã‚‹å›½çŽ‹ã§ã‚ã‚Šã€ãƒã‚¤ãƒ‰ãƒ©çŽ‹å›½å†…ã§ã¯ç›¸å½“äººæ°—ã®ã‚ã‚‹å­˜åœ¨ã ã€‚\\nå®Ÿéš›ä¿ºã¨åˆã‚ã¦ä¼šã£ãŸæ™‚ã‚‚å¤‰èº«é­”æ³•ã‚’ä½¿ã£ã¦ã„ãŸã—ã€ç´ ã®æ ¼å¥½ã§å¤–ã‚’å‡ºæ­©ãã¨å¤§å¤‰ãªã‚“ã ã‚ã†ã€‚\\nå¤‰è£…ã—ãŸãƒ©ã‚°ãƒŠã•ã‚“ã«ã¤ã„ã¦ç§»å‹•ã—ã¦ã€ã„ãã¤ã‹ã®è·¯åœ°ã‚’é€šã£ã¦äººæ°—ã®ãªã„æµ·è¾ºã«ã‚„ã£ã¦ãã‚‹ã¨ã€ãƒ©ã‚°ãƒŠã•ã‚“ã¯å¤‰è£…ã‚’è§£ã„ãŸã€‚\\nã€Œãµã…......ã‚«ã‚¤ãƒˆã€ã™ã¾ã‚“ãªã€‚è¿·æƒ‘ã‚’ã‹ã‘ãŸ......ãƒ•ã‚©ãƒ«ã‚¹!! è²´æ§˜ã€å‡ºæ­©ããªã¨è¨€ã†ãŸã˜ã‚ƒã‚ã†ãŒ!!ã€\\nã€ŒãŠã€è½ã¡ç€ããŸã¾ãˆãƒ©ã‚°ãƒŠ......ã¨ã€ã¨ã‚Šã‚ãˆãšã€æŒ¯ã‚Šä¸Šã’ãŸæ‹³ã‚’é™ã‚ã™ã‚“ã ã€‚ã„ã„ã‹ã€ã™ãã«æš´åŠ›ã«è¨´ãˆã‚‹ã®ã¯é‡Žè›®ãªç£ã®è¡Œã„ã ã€‚æˆ‘ã€…ã¯ç†æ€§ã‚ã‚‹ç”Ÿç‰©ã ......æ€’ã‚Šã«æµã•ã‚Œã¦ã¯ã„ã‘ãªã„ã€\\nã€Œã‚¢ãƒ¬ã ã‘å‰ã‚‚ã£ã¦é‡˜ã‚’åˆºã—ãŸã®ã«è¿·å­ã«ãªã£ã¦ãŠã£ã¦ã¯ã€æ€’ã‚Šã‚‚ã—ã‚ˆã†ã€‚ãã‚‚ãã‚‚......ã†ã‚“? ã»ã…......ãµã‚€......ã€\\nã€Œãªã‚“ã ã„? ãƒ©ã‚°ãƒŠã€å«Œãªç›®ã‚’ã—ã¦ã„ã‚‹ã­......ãã®ç›®ã¯ä¸»ã«ã€æ¶æ„ã£ãŸã‚Šã™ã‚‹éš›ã®ç›®ã ã‚ã†? ã„ã¾ãªãœãã®ã‚ˆã†ãªç›®ã‚’ç§ã«å¯¾ã—ã¦å‘ã‘ã¦ã„ã‚‹ã®ã‹ã€èª¬æ˜Žã‚’æ±‚ã‚ãŸã„ã¨ã“ã‚ã§ã¯ã‚ã‚‹ãŒ......æ‚²ã—ã„ã‹ãªè‹¥å¹²ã®å¿ƒå½“ãŸã‚ŠãŒã‚ã‚‹ã®ã§ã€ã‚ã¾ã‚Šè§¦ã‚Œã¦ã»ã—ããªã„ã¨ã„ã†æ°—æŒã¡ã‚‚ã‚ã‚‹ã€‚è§¦ã‚Œã‚‹ã®ã§ã‚ã‚Œã°ã€ã‚ã‚‹ç¨‹åº¦ã®è¦šæ‚Ÿã¯ã—ã¦ã„ãŸã ããŸã„ã­ã€\\nãªã«ã‚„ã‚‰ã‚ˆãåˆ†ã‹ã‚‰ãªã„ã‚„ã‚Šå–ã‚ŠãŒã‚ã£ã¦ã€æ€’ã£ã¦ã„ãŸãƒ©ã‚°ãƒŠã•ã‚“ã¯ã©ã“ã‹ãƒ‹ãƒ¤ãƒ‹ãƒ¤ã¨æ„åœ°ã®æ‚ªãã†ãªç¬‘ã¿ã‚’æµ®ã‹ã¹ã‚‹ã€‚\\nãã—ã¦ã€ãªã‚“ã¨ã‹è©±é¡Œã‚’æˆ»ãã†ã¨ã™ã‚‹ãƒ•ã‚©ãƒ«ã‚¹ã•ã‚“ã®æŠµæŠ—ã‚’ã‚ã–ç¬‘ã†ã‹ã®ã‚ˆã†ã«å£ã‚’é–‹ã„ãŸã€‚\\nã€Œ......ãƒ•ã‚©ãƒ«ã‚¹ã€ãŠä¸»çã—ãç…§ã‚Œã¦ãŠã‚‹ã‚ˆã†ã˜ã‚ƒã®? å¾®å¦™ã«é ¬ã«èµ¤ã¿ãŒè¦‹ãˆã‚‹ã€‚ã»ã»ã…ã€ã¾ã•ã‹ãŠä¸»ã®ãã‚“ãªé¡”ãŒè¦‹ã‚‰ã‚Œã‚‹ã¨ã¯......åŽŸå› ã¯ã€ã‚«ã‚¤ãƒˆã¨ã„ã†ã‚ã‘ã‹ã€‚ã¯ã¯ã¯ã€æµçŸ³ã¨è¤’ã‚ã‚‹ã¹ãã˜ã‚ƒãª! ã‚ã®ãƒ•ã‚©ãƒ«ã‚¹ãŒã€ãã‚“ãªé¡”â€•â€•ã†ã‰ã£!? ã¡ã‚‡ã£ã€å¾…ã¦ã£!?ã€\\nã€Œè­¦å‘Šã¯ã—ãŸã‚ˆ......ã€\\næ¥½ã—æ°—ã«ãƒ•ã‚©ãƒ«ã‚¹ã•ã‚“ã‚’æ¶æ„ã£ã¦ã„ãŸãƒ©ã‚°ãƒŠã•ã‚“ã ã£ãŸãŒã€çªå¦‚åœ°é¢ãŒã¾ã‚‹ã§æ²¼ã®ã‚ˆã†ã«å¤‰åŒ–ã—ã¦ä½“ãŒæ²ˆã¿ã€æŠµæŠ—ã—ã‚ˆã†ã¨ã™ã‚‹ã‚‚åœ°é¢ã«å¼•ããšã‚Šè¾¼ã¾ã‚Œã¦ã„ã£ãŸã€‚\\nãã—ã¦ã€ã„ã¤ã®é–“ã«ã‹æœ¨ã§ã§ããŸæ–ã‚’æŒã£ã¦ã„ãŸãƒ•ã‚©ãƒ«ã‚¹ã•ã‚“ãŒã€é­”åŠ›ã‚’è¾¼ã‚ã¦ãƒˆãƒ³ã¨åœ°é¢ã‚’å©ãã€‚\\nã€Œ......ã‚¢ãƒ¼ã‚¹ã‚·ã‚§ã‚¤ã‚«ãƒ¼ã€\\nã™ã‚‹ã¨ã€å¤§ããªæºã‚Œ......åœ°éœ‡ãŒç™ºç”Ÿã—ã¦ã€æ…Œã¦ã¦å‘¨å›²ã‚’è¦‹ã‚‹ãŒã€æµ·ã¯æ³¢ç«‹ã£ãŸã‚Šã—ã¦ãŠã‚‰ãš......ä¿ºãŸã¡ãŒã„ã‚‹å ´æ‰€ã ã‘æºã‚Œã¦ã„ã‚‹ã‚ˆã†ãªæ„Ÿã˜ã ã£ãŸã€‚\\nä¸æ€è­°ãªçŠ¶æ…‹ã ãŒã€ãŸã¶ã‚“å‘¨è¾ºã«è¢«å®³ãŒå‡ºãªã„ã‚ˆã†ã«å¯¾æŠ—è¡“å¼ã§èª¿æ•´ã—ã¦è¶…å±€åœ°çš„ãªåœ°éœ‡ã‚’èµ·ã“ã—ã¦ã„ã‚‹ã®ã ã‚ã†ã€‚\\nãã—ã¦å°‘ã—ã—ã¦ã€æºã‚ŒãŒåŽã¾ã‚‹ã¨......ãƒ©ã‚°ãƒŠã•ã‚“ãŒæ²ˆã‚“ã åœ°é¢ãŒé‹¼é‰„ã®ã‚ˆã†ã«ã‚¬ãƒƒãƒã‚¬ãƒã«å›ºã¾ã£ã¦ã„ãŸã€‚\\nã€Œ......ã•ã¦ã€ã›ã£ã‹ããƒã‚¤ãƒ‰ãƒ©çŽ‹å›½ã«æ¥ãŸã®ã ã—ã€çŽ‹åŸŽã«å‘ã‹ã†å‰ã«è»½ãæ˜¼é£Ÿã§ã‚‚é£Ÿã¹ã‚ˆã†ã‹ã€‚ä»Šå›žã®ç¤¼ã¨ã„ã†ã“ã¨ã§ã€æ˜¯éžå¥¢ã‚‰ã›ã¦ãã‚Œã€\\nã€Œ......ã‚ã€ã„ã‚„ã€ãƒ©ã‚°ãƒŠã•ã‚“ã¯?ã€\\nã€Œ......ã“ã®ç¨‹åº¦ã§æ­»ã¬ã‚ˆã†ãªã‚‰ã€åƒå¹´å‰ã®æ—…ã§æ­»ã‚“ã§ã„ã‚‹ã‚ˆã€‚æœ¬æ°—ã§å›ºã‚ãŸãŒã€æ•°åˆ†ã§å‡ºã¦ãã‚‹ã ã‚ã†ã­ã€‚ã¾ãã€æ”¾ã£ã¦ãŠã„ã¦ã„ã„ã•ã€è¡Œã“ã†ã€\\nã€Œã‚ã€ã¯ã„ã€', 'trg': \"Chapter 1627: Lost High Elf 8\\nOnce we reached the Teleportation Gate, we will soon be in Hydra Kingdom. Since today wasn't a season brimming with events, the gate wasn't so crowded, and we were able to teleport without much waiting time.\\nAs soon as we arrived at Hydra Kingdom, I could smell the unique seaside scent and feel the hustle and bustle of the place.\\n[No matter how many times I visit this place, Hydra Kingdom really is always bustling.]\\n[It's probably more accurate to call this place a huge port city. In fact, the markets near the sea are always quite lively. Hydra Kingdom has various innovative systems in place, particularly with their lower tax rate compared to others. The conditions are conducive for individuals to easily open shops, making it much easier for one to start a business. However, due to this nature of the Kingdom, there tends to be a large number of small to medium-sized shops and not many large commercial establishments.]\\n[I see, it certainly has that kind of atmosphere.]\\nHydra Kingdom is a place where the authority held by the citizens is strong, while the nobles are almost all in name only and their authority isn't much greater than the others. I even heard some saying that nobility in itself may eventually disappear in the country. I guess that country really seemed to be the most ahead of its time in the Human Realm.\\nAs someone from a different world, I find this characteristic of Hydra Kingdom's culture relatively easier to get used to compared to the other countries.\\nAs we were walking down the street while I had such a thought in mind, a familiar old man walked up to me from the other side. It was Laguna-san, wearing the disguise she had when we first met.\\n[Fors-san, it looks like she's come to pick you up.]\\n[Yeah, that indeed seems to be the case. She sure is as unskillful at Transformation Magic as usual...... Those well-versed in magic would notice the discrepancies fairly quickly. Well, she's a staunch front-line type, so I suppose it's inevitable......]\\n[Neun-san also mentioned how she's not very good at Recognition Inhibition Magic either.]\\n[Well, magic is something that some are good while others aren't. I might be referred to as a Sage, but it doesn't mean I can use all sorts of magic. One could say I'm more specialized in Nature Magic and Earth Magic. On the other hand, I'm not very skilled in Fire and other such magic.]\\nFors-san also seemed to notice Laguna-san immediately as she wryly smiled. Thereupon, Laguna-san seemed to know that we had noticed her, as she lightly signaled to us with a finger.\\nShe was probably signaling us to go to an unpopulated area in the direction her finger was pointing to. Laguna-san is a King with a tremendous approval rating and is quite popular within Hydra Kingdom.\\nIn fact, when I first met her, she was also using Transformation Magic, so I imagine it would be difficult for her to walk around in the street without a disguise.\\nWe followed the disguised Laguna-san through several alleys until we came to an unpopulated seaside area, where Laguna-san took off her disguise.\\n[Fuuu...... Kaito, sorry for troubling you...... Fors!!! I told you not to go out, didn't I!!!?]\\n[C- Calm down, Laguna...... F- For the time being, put down that fist you're raising. Listen here, it's the behavior of savage beasts to immediately resort to violence. We are rational creatures...... We must not let our anger get the better of us.]\\n[Even though I've kept on warning you about this repeatedly beforehand, for you to still get lost like that, I'd obviously get mad. In the first place...... Unnn? Hou...... fumu......]\\n[What is it? Laguna, that's an unpleasant look you have there...... Those eyes, don't you have those when you're teasing me? I'm now inclined to ask for an explanation for why those eyes are directed towards me at a time like this...... but unfortunately, I do have a slight inkling as to why, so I also feel like not wanting to delve into it too much. If you're gonna touch upon this matter, I'd appreciate holding onto some resolve to a certain extent.]\\nThey had some sort of exchange that I didn't quite understand, and Laguna-san, who should have been angry just now, is now grinning like a cheshire cat.\\nThen, as if to mock Fors-san's attempt to somehow get the topic back to before, Laguna-san spoke.\\n[......Fors, aren't you being unusually shy over there? Oh my, is that a faint blush on your cheeks I see? Hohou, who would have thought I'd get to see you looking like that...... The cause must be Kaito huh. Hahaha, I must say, as to be expected of him! For that Fors to be making such a face------- Whoa!? Hey, wait!?]\\n[I did warn you......]\\nLaguna-san was happily teasing Fors-san, but the ground beneath her feet suddenly turned into something like a swamp and her body sank, and even though she tried to resist, she was dragged into the ground.\\nThen, Fors-san, who was holding a wooden staff before I knew it, imbued it with magic power and tapped its base on the ground.\\n[......Earth Shaker.]\\nThereupon, a big tremor...... an earthquake occurred, and considering we were on the seaside, I looked around in panic, but then I noticed that the sea wasn't rippling at all...... It seemed that it was only the location we were in that were shaking.\\nIt's quite a mysterious situation, but most likely, to prevent any damage to the vicinity, a counteracting magic is being employed to induce an ultra-localized earthquake.\\nAnd then, a little later, when the shaking stopped...... the ground on which Laguna-san had sunk became as hard as steel.\\n[......Well then, since we've come all the way to Hydra Kingdom, let's have a light lunch before we head to the royal castle. As thanks for this matter, allow me to treat you.]\\n[......Ah, no, what about Laguna-san?]\\n[......If she were to die from something like this, she would have died on that journey a thousand years ago. I've seriously solidified and reinforced that ground, but I'm sure she'll be out in a few minutes. Well, just leave her be. Let's go.]\\n[Ah, yes.]\", 'meta': {'general': {'series_title_eng': 'I Was Caught up in a Hero Summoning, but That World Is at Peace (WN)', 'series_title_jap': 'å‹‡è€…å¬å–šã«å·»ãè¾¼ã¾ã‚ŒãŸã‘ã©ã€ç•°ä¸–ç•Œã¯å¹³å’Œã§ã—ãŸ', 'sentence_alignment_score': 1.53}, 'novelupdates': {'link': 'https://www.novelupdates.com/series/i-was-caught-up-in-a-hero-summoning-but-that-world-is-at-peace/', 'genres': ['Comedy', 'Fantasy', 'Harem', 'Romance', 'Shounen', 'Slice of Life'], 'tags': ['Adapted to Manga', 'Beast Companions', 'Beastkin', 'Beautiful Female Lead', 'Carefree Protagonist', 'Charismatic Protagonist', 'Charming Protagonist', 'Clingy Lover', 'Clumsy Love Interests', 'Demi-Humans', 'Demon Lord', 'Demons', 'Dense Protagonist', 'Devoted Love Interests', 'Divine Protection', 'Dragons', 'Easy Going Life', 'Elves', 'Fairies', 'First-time Interc**rse', 'Gambling', 'God-human Relationship', 'Goddesses', 'Heartwarming', 'Heroes', 'Human-Nonhuman Relationship', 'Immortals', 'Kind Love Interests', 'Lack of Common Sense', 'Loli', 'Love Interest Falls in Love First', 'Loyal Subordinates', 'Magic', 'Maids', 'Male Protagonist', 'Monsters', 'Multiple POV', 'Multiple Transported Individuals', 'Naive Protagonist', 'Nobles', 'Older Love Interests', 'Past Plays a Big Role', 'Polygamy', 'R-15', 'Royalty', 'Shy Characters', 'Store Owner', 'Strong Love Interests', 'Summoned Hero', 'Sword And Magic', 'Tomboyish Female Lead', 'Transported to Another World', 'Tsundere', 'Unconditional Love', 'Vampires', 'Wealthy Characters', 'World Tree', 'Yandere', 'Younger Love Interests'], 'rating': 4.1, 'rating_votes': 636}, 'syosetu': {'link': 'https://ncode.syosetu.com/n2273dh/', 'series_active': True, 'writer': 'ç¯å°', 'fav_novel_cnt': 75937, 'global_points': 246366}}, 'input_ids': [19191, 576, 13, 8755, 46831, 1773, 24, 45604, 19812, 29, 19224, 47010, 381, 443, 4258, 1713, 29, 43834, 1544, 7532, 459, 5832, 3754, 18, 13442, 13, 31691, 5654, 13, 53, 19812, 18, 2534, 29, 17200, 401, 816, 5618, 15168, 4286, 81, 9073, 794, 109, 11001, 45604, 375, 88, 5832, 5066, 15035, 7532, 29, 22493, 42, 1713, 29, 10670, 93, 3568, 3914, 13, 12942, 42, 58095, 39497, 36707, 83, 1708, 5832, 24, 18155, 261, 9419, 43834, 1544, 7532, 18, 267, 951, 5044, 109, 58132, 5938, 74, 207, 20056, 24, 18155, 11172, 9320, 18505, 3897, 42, 5751, 154, 13, 34, 2769, 1368, 1389, 207, 5832, 3454, 3568, 29, 1469, 6939, 8810, 18, 5743, 583, 7130, 58132, 5938, 3755, 194, 5832, 43834, 1544, 7532, 18, 13383, 15467, 34, 25696, 1046, 53, 15168, 11623, 29612, 13, 32512, 34, 4288, 29, 10891, 81, 19253, 9446, 53, 3840, 22, 1287, 11886, 29, 3484, 8636, 7062, 34, 11850, 83, 4734, 15168, 20243, 22, 1823, 8636, 13, 34, 5990, 207, 5832, 2855, 15168, 425, 19502, 598, 9446, 135, 311, 3420, 6787, 13, 29974, 3840, 34, 1446, 2243, 20712, 53, 15168, 797, 14870, 1486, 504, 29974, 398, 18, 10789, 16067, 29, 392, 207, 20056, 24, 18155, 42072, 15168, 4785, 29, 2534, 33936, 74, 207, 20056, 5066, 15035, 7532, 18, 11153, 13, 835, 34, 6976, 15168, 8050, 29, 44733, 18, 16325, 1450, 3565, 53, 7125, 16996, 22, 762, 882, 72, 504, 2525, 4661, 5832, 15091, 18, 44733, 11189, 34, 37438, 13, 295, 1295, 1618, 2970, 631, 81, 4734, 15168, 32400, 220, 6711, 13, 311, 427, 3535, 4385, 13, 26995, 22, 857, 8010, 326, 61, 104, 5832, 4126, 1270, 12554, 13, 1245, 2261, 18, 15168, 12081, 1046, 34332, 8636, 1287, 2685, 1368, 1389, 5832, 1021, 153, 22, 886, 3037, 1223, 22, 4720, 125, 512, 42, 15168, 11447, 135, 1011, 3162, 13, 392, 27942, 125, 647, 34, 4720, 381, 1086, 5832, 2689, 29, 1486, 104, 371, 13, 2423, 4068, 541, 36226, 1428, 647, 459, 5832, 24, 18155, 8245, 563, 647, 15168, 9645, 34, 1101, 1254, 74, 116, 20056, 24, 18155, 8007, 15168, 425, 326, 2463, 5832, 47794, 2423, 1874, 7063, 18, 1210, 594, 2463, 70, 70, 441, 2243, 29, 7063, 29, 23097, 398, 34, 1011, 714, 1713, 29, 6347, 4104, 3863, 29, 32852, 116, 5832, 193, 1385, 15168, 894, 18, 16789, 2543, 16789, 13, 910, 13580, 6038, 1304, 36705, 1386, 4258, 36705, 34, 70, 70, 20056, 24, 18155, 3117, 5316, 647, 109, 11514, 37073, 1550, 18, 24456, 83, 912, 193, 82, 207, 20056, 24, 18155, 193, 1385, 15168, 7063, 504, 398, 18, 2250, 594, 2363, 2250, 594, 420, 135, 207, 5832, 284, 109, 33128, 208, 1699, 504, 7456, 1450, 53, 4214, 618, 81, 18, 442, 34, 15168, 7186, 7063, 34, 9147, 4260, 504, 1868, 5654, 194, 5832, 6436, 7063, 261, 8563, 13, 7063, 29, 11294, 1709, 541, 42, 912, 370, 5832, 8050, 29, 2148, 1699, 13, 7063, 18, 24456, 2463, 20056, 14345, 563, 647, 109, 36226, 1428, 647, 229, 1713, 13825, 277, 326, 53, 14004, 1928, 22, 18251, 6298, 81, 277, 5832, 1434, 15168, 36226, 1428, 647, 13, 462, 109, 1245, 389, 34, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [29002, 652, 11366, 44, 20497, 3992, 5722, 1851, 360, 3401, 59, 3444, 5, 40611, 7900, 1600, 24716, 3, 59, 78, 1092, 48, 25, 53150, 660, 2, 1467, 508, 50, 145, 12, 52, 20, 6423, 18483, 31281, 55, 2676, 3, 5, 8188, 50, 145, 12, 52, 139, 16376, 3, 19, 59, 129, 734, 8, 22816, 7900, 635, 324, 2492, 164, 2, 407, 1092, 68, 59, 3193, 102, 53150, 660, 3, 23, 212, 6636, 5, 4736, 1927, 8660, 22322, 19, 567, 5, 43144, 19, 55167, 11, 5, 503, 2, 151, 3985, 839, 228, 264, 473, 23, 2325, 60, 503, 3, 53150, 660, 331, 32, 613, 49016, 2, 171, 151, 2244, 12, 17, 1601, 185, 4233, 8, 645, 60, 503, 20, 3300, 8847, 907, 2, 189, 727, 3, 5, 16210, 1522, 5, 1927, 58, 613, 1495, 28756, 2, 53150, 660, 143, 2450, 35490, 4422, 25, 503, 3, 3594, 55, 114, 4554, 6231, 3296, 5835, 8, 489, 2, 66, 2889, 58, 40753, 2992, 39, 2896, 8, 3485, 1174, 20506, 3, 867, 36, 324, 4606, 39, 105, 8, 984, 20, 1454, 2, 841, 3, 2245, 8, 60, 2779, 11, 5, 660, 3, 142, 17533, 8, 48, 20, 1288, 612, 11, 1028, 8, 12065, 146, 17, 3459, 20506, 19, 57, 264, 1288, 7127, 15375, 17, 2, 171, 151, 411, 215, 3, 36, 1534, 143, 30, 603, 11, 5867, 2, 171, 53150, 660, 32, 20, 503, 325, 5, 2354, 2037, 96, 5, 7101, 32, 1647, 3, 653, 5, 31897, 58, 1384, 97, 25, 426, 255, 19, 114, 2354, 1397, 12, 52, 324, 1866, 242, 5, 489, 2, 23, 276, 1016, 199, 897, 30, 47134, 25, 1514, 231, 3523, 10483, 25, 5, 1173, 2, 23, 2085, 30, 1173, 331, 3148, 8, 48, 5, 412, 2238, 11, 328, 164, 25, 5, 7684, 9479, 111, 2, 407, 805, 85, 20, 710, 268, 3, 23, 385, 60, 17050, 11, 53150, 660, 12, 17, 5651, 4075, 314, 4606, 8, 196, 505, 8, 5835, 8, 5, 243, 1670, 2, 407, 59, 129, 3034, 309, 5, 3358, 653, 23, 141, 288, 20, 591, 25, 709, 3, 20, 5233, 555, 248, 5019, 150, 8, 79, 85, 5, 243, 1176, 2, 106, 50, 56946, 3641, 146, 10026, 3, 5747, 5, 32881, 239, 141, 161, 59, 301, 1809, 2, 151, 4067, 17, 146, 10026, 3, 36, 1867, 160, 239, 12, 17, 291, 8, 3144, 27, 150, 2, 171, 151, 7232, 3, 30, 1426, 1733, 8, 48, 5, 855, 2, 320, 673, 32, 68, 1225, 17, 19475, 4007, 102, 35184, 1600, 33458, 68, 8259, 70, 70, 1364, 350, 146, 21742, 195, 25, 5939, 147, 3829, 5, 23713, 121, 6465, 39060, 11032, 2571, 2, 362, 3, 239, 12, 17, 20, 46401, 1899, 146, 5127, 2139, 3, 139, 23, 5974, 36, 12, 17, 17398, 70, 70, 171, 151, 1129, 463, 3675, 146, 10026, 270, 2620, 228, 239, 12, 17, 57, 298, 224, 102, 56523, 189, 7711, 475, 11839, 33458, 1627, 2, 171, 151, 9158, 3, 5939, 32, 351, 30, 199, 58, 224, 653, 489, 3544, 12, 52, 2, 23, 484, 48, 6339, 8, 68, 20, 48044, 3, 138, 36, 257, 145, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_data = train_data.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_train_data.save_to_disk(\"./tokenized_data\")\n",
    "\n",
    "print(tokenized_train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_22360\\3868523678.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments with evaluation disabled\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",  # Disable evaluation\n",
    "    save_strategy=\"steps\",  # Save checkpoints periodically\n",
    "    save_steps=500,  # Save a checkpoint every 500 steps\n",
    "    save_total_limit=3,  # Keep only the last 3 checkpoints\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use FP16 if a GPU is available\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,  # Training data\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14896/35793 [3:34:55<211:27:27, 36.43s/it]"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "original_model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "  0%|          | 0/35793 [00:00<?, ?it/s]c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15000/35793 [08:56<4:36:31,  1.25it/s]c:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[60715]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1695, 'grad_norm': 2.804398775100708, 'learning_rate': 1.1622384265079764e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15500/35793 [16:15<4:43:53,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1699, 'grad_norm': 2.7245330810546875, 'learning_rate': 1.1343000027938424e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16000/35793 [23:22<4:25:46,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1818, 'grad_norm': 3.016148090362549, 'learning_rate': 1.1063615790797084e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16500/35793 [30:29<4:29:14,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1462, 'grad_norm': 2.8308584690093994, 'learning_rate': 1.0784231553655745e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 17000/35793 [37:44<4:43:37,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1336, 'grad_norm': 2.7448885440826416, 'learning_rate': 1.0505406084988685e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17500/35793 [44:52<4:14:43,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1376, 'grad_norm': 2.608849048614502, 'learning_rate': 1.0226580616321628e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 18000/35793 [51:50<4:07:32,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1248, 'grad_norm': 3.023585081100464, 'learning_rate': 9.947196379180287e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18500/35793 [58:56<3:58:44,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1282, 'grad_norm': 2.806114912033081, 'learning_rate': 9.667812142038947e-06, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 19000/35793 [1:06:03<3:43:46,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1038, 'grad_norm': 2.6417856216430664, 'learning_rate': 9.388427904897607e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19500/35793 [1:13:06<3:54:52,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.103, 'grad_norm': 2.620089054107666, 'learning_rate': 9.109043667756265e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 20000/35793 [1:20:11<3:31:32,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0912, 'grad_norm': 2.9677674770355225, 'learning_rate': 8.829659430614926e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20500/35793 [1:27:15<3:31:13,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0821, 'grad_norm': 2.5534780025482178, 'learning_rate': 8.550275193473586e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 21000/35793 [1:34:19<3:26:34,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0814, 'grad_norm': 2.926218032836914, 'learning_rate': 8.270890956332244e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21500/35793 [1:41:27<3:20:08,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0825, 'grad_norm': 2.630681037902832, 'learning_rate': 7.991506719190904e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22000/35793 [1:48:27<3:06:31,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0786, 'grad_norm': 2.7724485397338867, 'learning_rate': 7.712681250523846e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22500/35793 [1:55:46<2:06:30,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0631, 'grad_norm': 2.579216480255127, 'learning_rate': 7.433855781856788e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23000/35793 [2:00:30<1:57:29,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0617, 'grad_norm': 2.6959280967712402, 'learning_rate': 7.154471544715448e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23500/35793 [2:05:13<1:53:22,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.059, 'grad_norm': 2.834404230117798, 'learning_rate': 6.875087307574107e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 24000/35793 [2:09:55<1:50:44,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0463, 'grad_norm': 2.6310675144195557, 'learning_rate': 6.595703070432767e-06, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24500/35793 [2:14:38<1:39:37,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0497, 'grad_norm': 2.972698450088501, 'learning_rate': 6.316318833291426e-06, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 25000/35793 [2:19:20<1:43:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0155, 'grad_norm': 2.623328924179077, 'learning_rate': 6.036934596150086e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 25500/35793 [2:24:06<1:35:22,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0197, 'grad_norm': 2.651944160461426, 'learning_rate': 5.757550359008745e-06, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 26000/35793 [2:28:46<1:33:18,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.013, 'grad_norm': 2.514033317565918, 'learning_rate': 5.478724890341688e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26500/35793 [2:33:27<1:26:46,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0212, 'grad_norm': 2.7133944034576416, 'learning_rate': 5.1993406532003465e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 27000/35793 [2:38:10<1:20:44,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.02, 'grad_norm': 2.6151833534240723, 'learning_rate': 4.919956416059007e-06, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27500/35793 [2:42:51<56:15,  2.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0149, 'grad_norm': 2.805305242538452, 'learning_rate': 4.640572178917666e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 28000/35793 [2:47:27<1:10:19,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0082, 'grad_norm': 2.6519854068756104, 'learning_rate': 4.361187941776325e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 28500/35793 [2:52:05<1:07:19,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0096, 'grad_norm': 2.5853476524353027, 'learning_rate': 4.082362473109268e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 29000/35793 [2:56:43<1:00:55,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0116, 'grad_norm': 2.951279878616333, 'learning_rate': 3.8029782359679268e-06, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 29500/35793 [3:01:21<58:55,  1.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.006, 'grad_norm': 2.7072432041168213, 'learning_rate': 3.5235939988265865e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 30000/35793 [3:05:57<51:43,  1.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0026, 'grad_norm': 2.556507110595703, 'learning_rate': 3.2442097616852458e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30500/35793 [3:10:34<48:50,  1.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9802, 'grad_norm': 2.8558201789855957, 'learning_rate': 2.9648255245439055e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 31000/35793 [3:15:11<46:04,  1.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9977, 'grad_norm': 2.7457668781280518, 'learning_rate': 2.685441287402565e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31500/35793 [3:19:47<38:52,  1.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0016, 'grad_norm': 2.637615442276001, 'learning_rate': 2.4060570502612245e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 32000/35793 [3:24:24<35:46,  1.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9958, 'grad_norm': 2.698451519012451, 'learning_rate': 2.126672813119884e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 32500/35793 [3:29:01<29:11,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.007, 'grad_norm': 2.9999020099639893, 'learning_rate': 1.8472885759785433e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33000/35793 [3:33:38<25:26,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0048, 'grad_norm': 2.849747657775879, 'learning_rate': 1.5679043388372028e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 33500/35793 [3:38:18<21:43,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9991, 'grad_norm': 2.555652618408203, 'learning_rate': 1.289078870170145e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34000/35793 [3:42:56<17:05,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9842, 'grad_norm': 2.5811784267425537, 'learning_rate': 1.0096946330288046e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34500/35793 [3:47:32<11:38,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9881, 'grad_norm': 2.5673716068267822, 'learning_rate': 7.303103958874641e-07, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 35000/35793 [3:52:04<07:16,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9847, 'grad_norm': 2.497488498687744, 'learning_rate': 4.509261587461236e-07, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 35500/35793 [3:56:40<02:39,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9825, 'grad_norm': 2.5490450859069824, 'learning_rate': 1.7154192160478308e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35793/35793 [3:59:23<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14363.9178, 'train_samples_per_second': 19.934, 'train_steps_per_second': 2.492, 'train_loss': 1.2199590257369632, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model\\\\tokenizer_config.json',\n",
       " './fine_tuned_model\\\\special_tokens_map.json',\n",
       " './fine_tuned_model\\\\vocab.json',\n",
       " './fine_tuned_model\\\\source.spm',\n",
       " './fine_tuned_model\\\\target.spm',\n",
       " './fine_tuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "original_model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 199.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m original_model \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m bleu, meteor \u001b[38;5;241m=\u001b[39m evaluate_model(original_model, tokenizer, test_data)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3163\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\envs\\EpubTranslatorEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 199.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "test_data = test_data.shuffle(seed=42).select(range(100))\n",
    "\n",
    "original_model = original_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleu, meteor = evaluate_model(original_model, tokenizer, test_data)\n",
    "\n",
    "print(f\"BLEU Score: {bleu:.2f}\")\n",
    "print(f\"METEOR Score: {meteor:.2f}\")\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_model\")\n",
    "finetuned_model = finetuned_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bleu, meteor = evaluate_model(finetuned_model, tokenizer, test_data)\n",
    "\n",
    "print(f\"BLEU Score: {bleu:.2f}\")\n",
    "print(f\"METEOR Score: {meteor:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EpubTranslatorEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
